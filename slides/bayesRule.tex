\documentclass{beamer}

\begin{document}

\section{Chapter 5: Bayes' Rule}


\begin{frame}{5.1 Bayes' Rule / Bayes’ Theorem}
  \begin{itemize}
    \item Introduction to Bayes' Rule.
    \end{itemize}
    Bayes' Rule, or Bayes' Theorem, is a fundamental concept in probability theory that provides a way to update probabilities based on new evidence or information.

    \begin{itemize}
    \item Formulation of Bayes' Theorem.
  \end{itemize}
  Bayes' Theorem is expressed as:
  \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \]
  where:
  \begin{itemize}
    \item $P(A|B)$ is the posterior probability.
    \item $P(B|A)$ is the likelihood.
    \item $P(A)$ is the prior probability.
    \item $P(B)$ is the evidence probability.
  \end{itemize}
\end{frame}


\begin{frame}{5.2 The Relationship between Likelihood and Bayes’}
  \begin{block}{Definition}
    The relationship between likelihood and Bayes' Rule lies in their collaboration to update probabilities based on new evidence. Likelihood provides the probability of observing the data given a particular hypothesis, while Bayes' Rule combines this likelihood with a prior probability to compute a posterior probability.
  \end{block}

  \begin{example}
    Consider a diagnostic test for a rare disease with a known false positive rate. The likelihood of observing a positive result in a healthy individual is determined by the test's accuracy. Bayes' Rule can then be applied to adjust the probability of having the disease based on the prior probability and the observed likelihood.
  \end{example}
\end{frame}

\begin{frame}{5.3 Applied Bayes' Rule to Parameters and Data}
  \begin{itemize}
    \item Practical applications of Bayes' Rule in handling parameters and data.
  \end{itemize}
\end{frame}



\begin{frame}{Handling Uncertainty}
  \begin{itemize}
    \item Bayesian framework naturally deals with uncertainty.
    \item \textbf{Credible Intervals:} Bayesian SORT OF/KINDA analog to confidence intervals.
    \item \textbf{Example:} Estimating a 95\% credible interval for the average response time in a system.
  \end{itemize}
\end{frame}

\begin{frame}{Decision Making with Bayes' Rule}
  \begin{itemize}
    \item Bayesian decision theory combines probabilities and utilities for optimal decision-making.
    \item \textbf{Loss Functions:} Quantify the cost of decisions.
    \item \textbf{Example:} Making decisions in a medical context based on diagnostic test results.
  \end{itemize}
\end{frame}

\begin{frame}{Computational Techniques}
  \begin{itemize}
    \item Bayesian computations may involve complex integrals.
    \item \textbf{Markov Chain Monte Carlo (MCMC):} Sampling from the posterior distribution.
    \item \textbf{Example:} Using MCMC to explore the posterior in a hierarchical model.
  \end{itemize}
\end{frame}

\begin{frame}{Challenges and Considerations}
  \begin{itemize}
    \item Discussing challenges such as choosing priors, model complexity, and computational demands.
    \item \textbf{Sensitivity Analysis:} Assessing the impact of different modeling choices.
    \item \textbf{Example:} Evaluating the sensitivity of Bayesian inference to different prior specifications.
  \end{itemize}
\end{frame}



\begin{frame}{5.4 Finding the Posterior Distribution in Closed Form}
  \begin{itemize}
    \item Techniques for deriving the posterior distribution in closed form.
  \end{itemize}
\end{frame}


\begin{frame}{Definition: Posterior Distribution in Closed Form}
  The posterior distribution in closed form refers to obtaining an analytical expression for the posterior distribution without the need for numerical methods. This involves applying specific techniques that allow for a mathematical solution.

  \vspace{0.5cm}

  In Bayesian statistics, having a closed-form expression for the posterior distribution can be advantageous as it provides insights into the distribution's shape and properties.

\end{frame}

\begin{frame}{Example: Gaussian Prior with Gaussian Likelihood}
  Let's consider a Bayesian model with a Gaussian prior and Gaussian likelihood. The prior distribution is \(P(\theta) \sim \mathcal{N}(\mu_0, \sigma_0^2)\), and the likelihood is \(P(D|\theta) \sim \mathcal{N}(\mu, \sigma^2)\).

  \vspace{0.5cm}

  Applying Bayes' Rule:
  \[
    P(\theta|D) = \frac{P(D|\theta) \cdot P(\theta)}{P(D)}
  \]

  If both prior and likelihood are Gaussian, it can be shown that the posterior is also a Gaussian distribution with updated parameters.

\end{frame}

\begin{frame}{Example: Gaussian Prior with Gaussian Likelihood}
  The posterior distribution for the model with Gaussian prior and likelihood is given by:
  \[
    P(\theta|D) \propto \exp\left(-\frac{1}{2} \frac{(\theta - \mu')^2}{\sigma'^2}\right)
  \]
  where \(\mu'\) and \(\sigma'^2\) are updated parameters based on the data.

  \vspace{0.5cm}

  This closed-form expression facilitates a direct understanding of how the data influences the posterior distribution.

\end{frame}



\begin{frame}{5.5 More about Prior Distributions}
  \begin{itemize}
    \item Further exploration of prior distributions in Bayesian inference.
  \end{itemize}
\end{frame}


\begin{frame}{Estimating Bias in a Coin: Introduction}
  \begin{block}{Definition}
    Estimating bias in a coin refers to determining the probability of the coin landing on heads or tails, indicating whether the coin is fair or biased.
  \end{block}
\end{frame}

\begin{frame}[fragile]{R Code: Bayesian Coin Bias Estimation}
\begin{verbatim}
# Bayesian Coin Bias Estimation [[A single coin]]

# Prior distribution for bias
prior_bias <- dbeta(seq(0, 1, by = 0.01), 2, 2)

# Likelihood function
likelihood <- function(data, bias) {
  heads <- sum(data == 'H')
  tails <- sum(data == 'T')
  return (dbinom(heads, heads + tails, bias))
}

# Posterior distribution
update_posterior <- function(prior, likelihood) {
  unnormalized_posterior <- prior * likelihood
  posterior <- unnormalized_posterior / sum(unnormalized_posterior)
  return (posterior)
}
\end{verbatim}
\end{frame}

\begin{frame}[fragile]{R Code: Bayesian Coin Bias Estimation}
\begin{verbatim}
# Simulate coin flips data
coin_flips_data <- c('H', 'T', 'H', 'H', 'T', 'T', 'H', 'H', 'H', 'T')

# Initial prior
prior <- dbeta(seq(0, 1, by = 0.01), 2, 2)

# Bayesian updating
for (flip in coin_flips_data) {
  likelihood_values <- likelihood(coin_flips_data, seq(0, 1, by = 0.01))
  posterior <- update_posterior(prior, likelihood_values)
  prior <- posterior
}

# Plot the results
plot(seq(0, 1, by = 0.01), prior, type = 'l', 
     xlab = 'Bias', ylab = 'Posterior Probability',
     main = 'Bayesian Coin Bias Estimation')
\end{verbatim}
\end{frame}

\begin{frame}{5.7 Why Bayesian Inference Can Be Difficult}
  \begin{itemize}
    \item Discussion on the challenges and complexities in Bayesian inference.
  \end{itemize}
\end{frame}

\end{document}

